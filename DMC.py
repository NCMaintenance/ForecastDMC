import streamlit as st
import pandas as pd
import numpy as np
import xgboost as xgb
import holidays
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import TimeSeriesSplit
from sklearn.feature_selection import SelectKBest, f_regression
from scipy.stats import pearsonr
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import warnings
from numba import jit, prange
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import multiprocessing as mp
from functools import partial
import time
warnings.filterwarnings('ignore')

#------------- SPEED OPTIMIZATIONS ---------------

@jit(nopython=True)
def fast_rolling_stats(values, window):
    """Fast rolling statistics using Numba"""
    n = len(values)
    if n == 0:
        return 0.0, 0.0, 0.0, 0.0, 0.0

    start_idx = max(0, n - window)
    window_data = values[start_idx:n]
    if len(window_data) == 0:
        return 0.0, 0.0, 0.0, 0.0, 0.0
    
    mean_val = np.mean(window_data)
    std_val = np.std(window_data) if len(window_data) > 1 else 0.0
    max_val = np.max(window_data)
    min_val = np.min(window_data)
    median_val = np.median(window_data)
    return mean_val, std_val, max_val, min_val, median_val

@jit(nopython=True)
def fast_ema(values, alpha):
    """Fast exponential moving average"""
    if len(values) == 0:
        return 0.0

    ema = values[0]
    for i in range(1, len(values)):
        ema = alpha * values[i] + (1 - alpha) * ema
    return ema

@jit(nopython=True)
def fast_pct_change(values, periods):
    """Fast percentage change calculation"""
    n = len(values)
    if n <= periods or values[n-periods-1] == 0:
        return 0.0
    return (values[n-1] - values[n-periods-1]) / values[n-periods-1]

@jit(nopython=True)
def fast_volatility(values, window):
    """Fast volatility calculation"""
    n = len(values)
    if n < window:
        window_data = values
    else:
        window_data = values[n-window:n]

    if len(window_data) <= 1:
        return 0.0
    
    mean_val = np.mean(window_data)
    if mean_val == 0:
        return 0.0
        
    std_val = np.std(window_data)
    return std_val / mean_val

@jit(nopython=True)
def fast_trend_calculation(values, window=7):
    """Calculate trend slope over a window"""
    n = len(values)
    if n < window:
        window_data = values
        x_data = np.arange(len(window_data))
    else:
        window_data = values[n-window:n]
        x_data = np.arange(window)

    if len(window_data) < 2:
        return 0.0
    
    # Simple linear regression slope
    n_points = len(window_data)
    sum_x = np.sum(x_data)
    sum_y = np.sum(window_data)
    sum_xy = np.sum(x_data * window_data)
    sum_x2 = np.sum(x_data * x_data)
    denominator = n_points * sum_x2 - sum_x * sum_x
    
    if denominator == 0:
        return 0.0
        
    slope = (n_points * sum_xy - sum_x * sum_y) / denominator
    return slope

#------------- CORRELATION ANALYSIS AND FEATURE SELECTION ---------------

def analyze_feature_correlations(feature_df, target_col='y', correlation_threshold=0.1):
    """Analyze correlations and select best features"""

    # Calculate all features first
    all_features = []
    # Basic temporal features
    temporal_features = [
        'year', 'month', 'day', 'dayofweek', 'hour', 'week_of_year', 'quarter', 
        'day_of_year', 'hour_sin', 'hour_cos', 'dow_sin', 'dow_cos', 'month_sin', 
        'month_cos', 'quarter_sin', 'quarter_cos'
    ]
    all_features.extend(temporal_features)

    # Pattern features
    pattern_features = [
        'is_weekend', 'is_monday', 'is_friday', 'is_spring', 'is_summer', 
        'is_autumn', 'is_winter', 'weekend_evening', 'friday_evening'
    ]
    all_features.extend(pattern_features)

    # All possible lag features (1-30 days)
    lag_features = [f'y_lag{i}' for i in range(1, 31)]
    all_features.extend(lag_features)

    # All possible rolling features
    rolling_windows = [3, 7, 14, 21, 30, 60, 90]
    rolling_stats = ['mean', 'std', 'max', 'min', 'median']
    for window in rolling_windows:
        for stat in rolling_stats:
            all_features.append(f'y_rolling_{stat}_{window}')

    # EMA features
    ema_alphas = [0.1, 0.2, 0.3, 0.5, 0.7, 0.9]
    for alpha in ema_alphas:
        all_features.append(f'y_ema_{alpha}')

    # Percentage change features
    pct_periods = [1, 2, 3, 7, 14, 21, 30]
    for period in pct_periods:
        all_features.append(f'y_pct_change_{period}d')

    # Volatility features
    vol_windows = [7, 14, 30]
    for window in vol_windows:
        all_features.append(f'y_volatility_{window}')

    # Trend features
    trend_windows = [3, 7, 14, 21]
    for window in trend_windows:
        all_features.append(f'y_trend_{window}')

    # Other features
    other_features = ['is_holiday', 'trend']
    all_features.extend(other_features)

    # Check which features exist in the dataframe
    available_features = [f for f in all_features if f in feature_df.columns]
    
    # Calculate correlations
    correlations = {}
    target_values = feature_df[target_col].dropna()
    for feature in available_features:
        if feature in feature_df.columns:
            feature_values = feature_df[feature]
            # Align the data (remove NaN pairs)
            aligned_data = pd.DataFrame({
                'target': target_values,
                'feature': feature_values
            }).dropna()

            if len(aligned_data) > 10:  # Need sufficient data points
                try:
                    corr, p_value = pearsonr(aligned_data['target'], aligned_data['feature'])
                    correlations[feature] = {
                        'correlation': abs(corr),
                        'p_value': p_value,
                        'raw_correlation': corr
                    }
                except:
                    correlations[feature] = {
                        'correlation': 0.0,
                        'p_value': 1.0,
                        'raw_correlation': 0.0
                    }

    # Sort by correlation strength
    sorted_features = sorted(correlations.items(), key=lambda x: x[1]['correlation'], reverse=True)
    
    # Select features above threshold
    selected_features = []
    correlation_info = []
    for feature, corr_info in sorted_features:
        if corr_info['correlation'] >= correlation_threshold and corr_info['p_value'] < 0.05:
            selected_features.append(feature)
            correlation_info.append({
                'feature': feature,
                'correlation': corr_info['raw_correlation'],
                'abs_correlation': corr_info['correlation'],
                'p_value': corr_info['p_value']
            })

    # Ensure we have at least some essential features
    essential_features = ['y_lag1', 'y_lag7', 'dayofweek', 'hour', 'month']
    for feature in essential_features:
        if feature in available_features and feature not in selected_features:
            selected_features.append(feature)
            if feature in correlations:
                correlation_info.append({
                    'feature': feature,
                    'correlation': correlations[feature]['raw_correlation'],
                    'abs_correlation': correlations[feature]['correlation'],
                    'p_value': correlations[feature]['p_value']
                })
                
    return selected_features[:50], pd.DataFrame(correlation_info)  # Limit to top 50 features

@st.cache_data
def get_ireland_holidays(years):
    """Cached holiday calculation"""
    ir_holidays = holidays.Ireland(years=years)
    for year in years:
        ir_holidays[f'{year}-02-15'] = "Custom Feb Bank Holiday"
    return ir_holidays

def create_temporal_features(dates):
    """Vectorized temporal feature creation"""
    df = pd.DataFrame(index=dates)

    # Basic temporal features
    df['year'] = dates.year
    df['month'] = dates.month
    df['day'] = dates.day
    df['dayofweek'] = dates.dayofweek
    df['hour'] = dates.hour
    df['week_of_year'] = dates.isocalendar().week.astype(int)
    df['quarter'] = dates.quarter
    df['day_of_year'] = dates.dayofyear

    # Cyclical encoding
    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)
    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)
    df['dow_sin'] = np.sin(2 * np.pi * df['dayofweek'] / 7)
    df['dow_cos'] = np.cos(2 * np.pi * df['dayofweek'] / 7)
    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)
    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)
    df['quarter_sin'] = np.sin(2 * np.pi * df['quarter'] / 4)
    df['quarter_cos'] = np.cos(2 * np.pi * df['quarter'] / 4)

    # Pattern features
    df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)
    df['is_monday'] = (df['dayofweek'] == 0).astype(int)
    df['is_friday'] = (df['dayofweek'] == 4).astype(int)
    
    # Seasonal patterns
    df['is_spring'] = df['month'].isin([3, 4, 5]).astype(int)
    df['is_summer'] = df['month'].isin([6, 7, 8]).astype(int)
    df['is_autumn'] = df['month'].isin([9, 10, 11]).astype(int)
    df['is_winter'] = df['month'].isin([12, 1, 2]).astype(int)

    # Interaction features
    df['weekend_evening'] = df['is_weekend'] * (df['hour'] >= 18).astype(int)
    df['friday_evening'] = df['is_friday'] * (df['hour'] >= 17).astype(int)

    return df

@st.cache_data
def create_comprehensive_features(df, metric, ir_holidays):
    """Create comprehensive feature set for correlation analysis"""
    temp_df = df.copy()
    temp_df['ds'] = pd.to_datetime(temp_df['Date'])
    feature_df = temp_df[['ds', metric]].rename(columns={metric: 'y'}).dropna().set_index('ds')

    # Create temporal features
    temporal_features = create_temporal_features(feature_df.index)
    feature_df = pd.concat([feature_df, temporal_features], axis=1)

    # Create ALL possible lag features for lag in range(1, 31): # 1 to 30 days
    for lag in range(1, 31):
        feature_df[f'y_lag{lag}'] = feature_df['y'].shift(lag)

    # Create ALL possible rolling features
    rolling_windows = [3, 7, 14, 21, 30, 60, 90]
    for window in rolling_windows:
        feature_df[f'y_rolling_mean_{window}'] = feature_df['y'].rolling(window, min_periods=1).mean()
        feature_df[f'y_rolling_std_{window}'] = feature_df['y'].rolling(window, min_periods=1).std()
        feature_df[f'y_rolling_max_{window}'] = feature_df['y'].rolling(window, min_periods=1).max()
        feature_df[f'y_rolling_min_{window}'] = feature_df['y'].rolling(window, min_periods=1).min()
        feature_df[f'y_rolling_median_{window}'] = feature_df['y'].rolling(window, min_periods=1).median()

    # Create ALL EMA features
    ema_alphas = [0.1, 0.2, 0.3, 0.5, 0.7, 0.9]
    for alpha in ema_alphas:
        feature_df[f'y_ema_{alpha}'] = feature_df['y'].ewm(alpha=alpha).mean()

    # Create ALL percentage change features
    pct_periods = [1, 2, 3, 7, 14, 21, 30]
    for period in pct_periods:
        feature_df[f'y_pct_change_{period}d'] = feature_df['y'].pct_change(period)

    # Create volatility features
    vol_windows = [7, 14, 30]
    for window in vol_windows:
        rolling_std = feature_df['y'].rolling(window, min_periods=1).std()
        rolling_mean = feature_df['y'].rolling(window, min_periods=1).mean()
        feature_df[f'y_volatility_{window}'] = rolling_std / (rolling_mean + 1e-8)

    # Create trend features (slope over different windows)
    trend_windows = [3, 7, 14, 21]
    for window in trend_windows:
        feature_df[f'y_trend_{window}'] = feature_df['y'].rolling(window, min_periods=2).apply(
            lambda x: np.polyfit(range(len(x)), x, 1)[0] if len(x) >= 2 else 0
        )
    
    # Holiday effects
    holiday_dates = set(pd.to_datetime(list(ir_holidays.keys())).date)
    if hasattr(feature_df.index, 'date'):
        date_series = pd.Series(feature_df.index.date, index=feature_df.index)
    else:
        date_series = pd.Series([d.date() for d in feature_df.index], index=feature_df.index)
    feature_df['is_holiday'] = date_series.map(lambda x: 1 if x in holiday_dates else 0)

    # Global trend
    feature_df['trend'] = np.arange(len(feature_df))

    # Clean up NaN values
    feature_df = feature_df.replace([np.inf, -np.inf], np.nan)

    # Fill NaN values strategically
    numeric_cols = feature_df.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        if col == 'y':
            continue
        if 'pct_change' in col or 'trend' in col:
            feature_df[col] = feature_df[col].fillna(0)
        else:
            feature_df[col] = feature_df[col].fillna(feature_df[col].median())
            
    return feature_df.reset_index()

@st.cache_data
def train_enhanced_xgboost(feature_df, metric_name, correlation_threshold=0.1):
    """Enhanced XGBoost training with correlation-based feature selection"""
    
    # Perform correlation analysis and feature selection
    selected_features, correlation_info = analyze_feature_correlations(
        feature_df, 'y', correlation_threshold
    )

    if len(feature_df) < 30:
        raise ValueError(f"Insufficient data: {len(feature_df)} rows")

    # Prepare data with selected features
    model_data = feature_df[['ds', 'y'] + selected_features].copy()
    model_data = model_data.dropna()

    if len(model_data) < 20:
        raise ValueError("Too much data lost after cleaning")

    X = model_data[selected_features]
    y = model_data['y']

    # Scale features
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # Enhanced XGBoost parameters
    xgb_model = xgb.XGBRegressor(
        n_estimators=200,
        max_depth=7,
        learning_rate=0.08,
        subsample=0.8,
        colsample_bytree=0.8,
        reg_alpha=0.1,
        reg_lambda=0.1,
        random_state=42,
        n_jobs=-1,
        tree_method='hist',
        early_stopping_rounds=20
    )

    # Train/validation split
    train_size = int(0.85 * len(X_scaled))
    X_train, X_val = X_scaled[:train_size], X_scaled[train_size:]
    y_train, y_val = y[:train_size], y[train_size:]
    
    # Train with early stopping
    xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)

    return xgb_model, scaler, selected_features, model_data, correlation_info

#------------- TREND-AWARE ITERATIVE FORECASTING ---------------

def trend_aware_iterative_forecast(xgb_model, scaler, features, model_data, forecast_periods=14, ir_holidays=None):
    """Enhanced iterative forecasting that maintains trends"""
    
    forecasts = np.zeros(forecast_periods)
    forecast_dates = []
    
    historical_values = model_data['y'].values
    last_date = pd.to_datetime(model_data['ds'].iloc[-1])
    
    recent_window = min(14, len(historical_values))
    recent_trend = fast_trend_calculation(historical_values, recent_window)
    recent_volatility = np.std(historical_values[-recent_window:]) if recent_window > 1 else 0
    
    working_values = historical_values.copy()
    master_history_df = model_data[['ds', 'y']].copy()
    
    base_patterns = {}
    if len(historical_values) >= 7:
        for dow in range(7):
            dow_values = [historical_values[i] for i, dt in enumerate(model_data['ds']) if pd.to_datetime(dt).dayofweek == dow]
            if dow_values:
                base_patterns[f'dow_{dow}'] = np.mean(dow_values)

    for i in range(forecast_periods):
        next_date = last_date + pd.Timedelta(days=1)
        next_features_dict = {}
        
        temp_dates_for_feature_calc = pd.DatetimeIndex([next_date])
        temporal_df_for_step = create_temporal_features(temp_dates_for_feature_calc)

        for feature_name in features:
            if feature_name in temporal_df_for_step.columns:
                next_features_dict[feature_name] = temporal_df_for_step[feature_name].iloc[0]
            
            elif feature_name.startswith('y_lag'):
                lag = int(feature_name.replace('y_lag', ''))
                next_features_dict[feature_name] = working_values[-lag] if lag <= len(working_values) else np.nan
            
            # --- START OF FIX ---
            # This block handles rolling statistics. The original error occurs when the
            # feature name is malformed (e.g., 'y_rolling_7_min' instead of 'y_rolling_min_7').
            # The code now explicitly checks if the window part of the name is a digit
            # before trying to convert it to an integer. This prevents the ValueError.
            elif feature_name.startswith('y_rolling_'):
                parts = feature_name.split('_')
                # Expected format: y_rolling_{stat}_{window}, e.g., ['y', 'rolling', 'mean', '7']
                if len(parts) == 4 and parts[3].isdigit():
                    window_val = int(parts[3])
                    stat_type = parts[2]
                    
                    mean_val, std_val, max_val, min_val, median_val = fast_rolling_stats(working_values, window_val)
                    
                    if stat_type == 'mean':
                        next_features_dict[feature_name] = mean_val
                    elif stat_type == 'std':
                        next_features_dict[feature_name] = std_val
                    elif stat_type == 'max':
                        next_features_dict[feature_name] = max_val
                    elif stat_type == 'min':
                        next_features_dict[feature_name] = min_val
                    elif stat_type == 'median':
                        next_features_dict[feature_name] = median_val
                    else:
                        next_features_dict[feature_name] = np.nan # Fallback for unknown stat
                else:
                    # If format is wrong (e.g., 'y_rolling_7_min'), assign NaN and avoid a crash
                    next_features_dict[feature_name] = np.nan
            # --- END OF FIX ---
            
            elif feature_name.startswith('y_ema_'):
                alpha_val = float(feature_name.replace('y_ema_', ''))
                next_features_dict[feature_name] = fast_ema(working_values, alpha_val) if len(working_values) > 0 else np.nan
            
            elif feature_name.startswith('y_pct_change_'):
                period_val = int(feature_name.replace('y_pct_change_', '').replace('d', ''))
                next_features_dict[feature_name] = fast_pct_change(working_values, period_val) if len(working_values) > period_val else 0.0
            
            elif feature_name.startswith('y_volatility_'):
                window_val = int(feature_name.replace('y_volatility_', ''))
                next_features_dict[feature_name] = fast_volatility(working_values, window_val)

            elif feature_name.startswith('y_trend_'):
                window_val = int(feature_name.replace('y_trend_', ''))
                next_features_dict[feature_name] = fast_trend_calculation(working_values, window_val)

            elif feature_name == 'trend':
                next_features_dict[feature_name] = len(master_history_df)
            
            elif feature_name == 'is_holiday':
                next_features_dict[feature_name] = 1 if ir_holidays and next_date.date() in ir_holidays else 0
        
        feature_vector_list = [next_features_dict.get(f, np.nan) for f in features]
        imputed_value = np.nanmedian(working_values) if len(working_values) > 0 else 0
        feature_vector_filled = [imputed_value if np.isnan(x) else x for x in feature_vector_list]
        feature_vector = np.array(feature_vector_filled).reshape(1, -1)

        feature_vector_scaled = scaler.transform(feature_vector)
        base_prediction = xgb_model.predict(feature_vector_scaled)[0]

        if i == 0:
            trend_adjustment = recent_trend * 0.5
        else:
            model_trend = forecasts[i-1] - (forecasts[i-2] if i > 1 else working_values[-1])
            trend_adjustment = 0.7 * model_trend + 0.3 * recent_trend
        
        adjusted_prediction = base_prediction + trend_adjustment
        
        dow_pattern_key = f'dow_{next_date.dayofweek}'
        if dow_pattern_key in base_patterns:
            pattern_adjustment = 0.1 * (base_patterns[dow_pattern_key] - np.mean(historical_values))
            adjusted_prediction += pattern_adjustment

        recent_mean = np.mean(working_values[-7:]) if len(working_values) >= 7 else np.mean(working_values)
        lower_bound = recent_mean - 3 * recent_volatility
        upper_bound = recent_mean + 3 * recent_volatility
        
        final_prediction = max(0, min(upper_bound, max(lower_bound, adjusted_prediction)))
        
        working_values = np.append(working_values, final_prediction)
        new_row_for_master = pd.DataFrame({'ds': [next_date], 'y': [final_prediction]})
        master_history_df = pd.concat([master_history_df, new_row_for_master], ignore_index=True)

        forecasts[i] = final_prediction
        forecast_dates.append(next_date)
        last_date = next_date
        
    return forecasts.tolist(), forecast_dates

#------------- EVALUATION AND VISUALIZATION ---------------

def enhanced_evaluate_model(xgb_model, scaler, features, model_data):
    """Enhanced model evaluation with feature importance"""
    test_size = min(14, len(model_data) // 4)
    test_size = max(3, test_size) if len(model_data) > 3 else len(model_data)

    test_data = model_data.tail(test_size).copy()
    X_test = test_data[features].fillna(0)
    X_test_scaled = scaler.transform(X_test)
    y_true = test_data['y'].values
    y_pred = xgb_model.predict(X_test_scaled)

    # Calculate metrics
    mae = np.mean(np.abs(y_true - y_pred))
    rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))
    mape = np.mean(np.abs((y_true - y_pred) / np.maximum(y_true, 1e-6))) * 100

    # Feature importance
    feature_importance = pd.DataFrame({
        'feature': features,
        'importance': xgb_model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    metrics = {'MAE': mae, 'RMSE': rmse, 'MAPE': mape}
    return metrics, y_true, y_pred, test_data['ds'].values, feature_importance

def create_enhanced_plot(historical_dates, historical_values, test_pred_dates, test_pred_values,
                         forecast_dates, forecast_values, metric_name):
    """Enhanced plotting with trend analysis"""
    fig = make_subplots(
        rows=2, cols=1,
        subplot_titles=(f'{metric_name} - Forecast Results', 'Recent Trend Analysis'),
        vertical_spacing=0.12,
        row_heights=[0.7, 0.3]
    )

    # Main forecast plot
    fig.add_trace(go.Scatter(
        x=historical_dates, y=historical_values, mode='lines', name='Historical Data',
        line=dict(color='blue', width=2)
    ), row=1, col=1)

    # Test predictions
    if len(test_pred_dates) > 0:
        fig.add_trace(go.Scatter(
            x=test_pred_dates, y=test_pred_values, mode='lines+markers', name='Test Predictions',
            line=dict(color='orange', width=2, dash='dash'), marker=dict(size=6)
        ), row=1, col=1)

    # Forecast
    fig.add_trace(go.Scatter(
        x=forecast_dates, y=forecast_values, mode='lines+markers', name='Forecast',
        line=dict(color='red', width=3), marker=dict(size=8, symbol='diamond')
    ), row=1, col=1)

    # Recent trend analysis (last 30 days)
    recent_days = min(30, len(historical_values))
    recent_dates = historical_dates[-recent_days:]
    recent_values = historical_values[-recent_days:]
    
    fig.add_trace(go.Scatter(
        x=recent_dates, y=recent_values, mode='lines+markers', name='Recent Trend',
        line=dict(color='green', width=2), marker=dict(size=4)
    ), row=2, col=1)

    # Add trend line
    if len(recent_values) >= 2:
        x_numeric = np.arange(len(recent_values))
        trend_coef = np.polyfit(x_numeric, recent_values, 1)
        trend_line = np.poly1d(trend_coef)(x_numeric)
        fig.add_trace(go.Scatter(
            x=recent_dates, y=trend_line, mode='lines', name='Trend Line',
            line=dict(color='purple', width=2, dash='dot')
        ), row=2, col=1)

    # Layout
    fig.update_layout(
        height=700,
        title_text=f"Enhanced {metric_name} Forecast Analysis",
        showlegend=True,
        hovermode='x unified'
    )
    fig.update_xaxes(title_text="Date", row=2, col=1)
    fig.update_yaxes(title_text=metric_name, row=1, col=1)
    fig.update_yaxes(title_text="Recent Values", row=2, col=1)

    return fig

#------------- STREAMLIT APPLICATION ---------------

def main():
    st.set_page_config(page_title="Enhanced Time Series Forecaster", layout="wide")

    st.title("ðŸš€ Enhanced Time Series Forecaster")
    st.markdown("Advanced XGBoost forecasting with correlation analysis and feature selection")

    # File upload
    uploaded_file = st.file_uploader("Upload CSV file", type=['csv'])

    if uploaded_file is not None:
        try:
            # Load data
            df = pd.read_csv(uploaded_file)
            st.success(f"Data loaded: {len(df)} rows, {len(df.columns)} columns")
            
            # Display data info
            with st.expander("Data Preview"):
                st.dataframe(df.head())
                st.write("**Columns:**", list(df.columns))

            # Column selection
            col1, col2 = st.columns(2)
            with col1:
                date_col = st.selectbox("Select Date Column", df.columns)
            with col2:
                metric_col = st.selectbox("Select Metric Column", [col for col in df.columns if col != date_col])

            # Parameters
            with st.sidebar:
                st.header("Forecasting Parameters")
                forecast_days = st.slider("Forecast Days", 1, 30, 14)
                correlation_threshold = st.slider("Correlation Threshold", 0.05, 0.3, 0.1, 0.01)
                st.header("Model Parameters")
                show_correlations = st.checkbox("Show Feature Correlations", True)
                show_importance = st.checkbox("Show Feature Importance", True)
            
            if st.button("ðŸ”® Generate Forecast", type="primary"):
                with st.spinner("Processing... This may take a moment"):
                    # Prepare data
                    df_clean = df[[date_col, metric_col]].copy()
                    df_clean.columns = ['Date', metric_col]
                    df_clean['Date'] = pd.to_datetime(df_clean['Date'])
                    df_clean = df_clean.dropna().sort_values('Date')

                    if len(df_clean) < 30:
                        st.error("Need at least 30 data points for forecasting")
                        return

                    # Get holidays
                    min_year = df_clean['Date'].dt.year.min()
                    max_year = df_clean['Date'].dt.year.max() + 2
                    years = list(range(min_year, max_year))
                    ir_holidays = get_ireland_holidays(years)
                    
                    # Create features
                    feature_df = create_comprehensive_features(df_clean, metric_col, ir_holidays)
                    
                    # Train model
                    xgb_model, scaler, selected_features, model_data, correlation_info = train_enhanced_xgboost(
                        feature_df, metric_col, correlation_threshold
                    )
                    
                    # Evaluate model
                    metrics, y_true, y_pred, test_dates, feature_importance = enhanced_evaluate_model(
                        xgb_model, scaler, selected_features, model_data
                    )
                    
                    # Generate forecast
                    forecast_values, forecast_dates = trend_aware_iterative_forecast(
                        xgb_model, scaler, selected_features, model_data, forecast_days, ir_holidays
                    )
                    
                    # Display results
                    st.success("âœ… Forecast completed!")
                    
                    # Metrics
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("MAE", f"{metrics['MAE']:.3f}")
                    with col2:
                        st.metric("RMSE", f"{metrics['RMSE']:.3f}")
                    with col3:
                        st.metric("MAPE", f"{metrics['MAPE']:.1f}%")

                    # Plot
                    historical_dates = pd.to_datetime(model_data['ds']).tolist()
                    historical_values = model_data['y'].tolist()
                    fig = create_enhanced_plot(
                        historical_dates, historical_values,
                        pd.to_datetime(test_dates).tolist(), y_pred.tolist(),
                        forecast_dates, forecast_values, metric_col
                    )
                    st.plotly_chart(fig, use_container_width=True)

                    # Additional analysis
                    if show_correlations and not correlation_info.empty:
                        with st.expander("ðŸ”— Feature Correlations"):
                            st.dataframe(correlation_info.head(20))

                    if show_importance and not feature_importance.empty:
                        with st.expander("â­ Feature Importance"):
                            st.dataframe(feature_importance.head(15))
                    
                    # Forecast table
                    with st.expander("ðŸ“Š Forecast Details"):
                        forecast_df = pd.DataFrame({
                            'Date': forecast_dates,
                            'Forecast': forecast_values
                        })
                        st.dataframe(forecast_df)
                    
                    # Download button
                    csv = forecast_df.to_csv(index=False).encode('utf-8')
                    st.download_button(
                        "Download Forecast CSV",
                        csv,
                        "forecast.csv",
                        "text/csv",
                        key='download-csv'
                    )
                    
                    st.info(f"âœ¨ Used {len(selected_features)} selected features out of {len(feature_df.columns)-2} total features")

        except Exception as e:
            st.error(f"An error occurred: {str(e)}")
            st.exception(e) # This will print the full traceback in the app
    else:
        st.info("ðŸ‘† Upload a CSV file to get started")
        # Example data format
        with st.expander("ðŸ“‹ Expected Data Format"):
            st.write("""
Your CSV should have at a minimum:
- A date column (most standard date/datetime formats are accepted).
- A numeric metric column to forecast.

**Example:**

Date,Sales
2023-01-01,100
2023-01-02,120
2023-01-03,95
...
            """)

if __name__ == "__main__":
    main()

